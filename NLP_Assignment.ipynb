{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gen-AI Bootcamp 24** *Natural Language Processing Course Assignment*\n",
    "\n",
    "_________________________________________________________________________\n",
    "\n",
    "## **Part 1: Text Collection and Loading**\n",
    "**Objective:** *Collect and load a text dataset from a selected domain into a suitable format for\n",
    "processing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Domain**: *Automotive* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kaggle Dataset**: *https://www.kaggle.com/datasets/ankkur13/edmundsconsumer-car-ratings-and-reviews?select=Scraped_Car_Review_dodge.csv*\n",
    "This is a dataset containing consumer's thought and the star rating of car manufacturer/model/type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loading Dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_csv('Scraped_Car_Review_dodge.csv')\n",
    "# Print the shape of the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Displaying the first few rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>on 07/17/05 21:59 PM (PDT)</td>\n",
       "      <td>Mark</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 3500 3dr Ext Van (5.2...</td>\n",
       "      <td>Disappointmnet</td>\n",
       "      <td>Bought this car as a commuter vehicle for a v...</td>\n",
       "      <td>2.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>on 07/16/02 00:00 AM (PDT)</td>\n",
       "      <td>Tom Sheer</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 3500 Maxi 3dr Ext Van...</td>\n",
       "      <td>Sweet van</td>\n",
       "      <td>This van rocks its the best, lots of room. I ...</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>on 12/29/07 21:57 PM (PST)</td>\n",
       "      <td>Keven Smith</td>\n",
       "      <td>2001 Dodge Ram Cargo Van 2500 Maxi 3dr Ext Van...</td>\n",
       "      <td>Keven Smith</td>\n",
       "      <td>Great work vehicle. Drives nice. has lots of ...</td>\n",
       "      <td>4.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>on 02/09/05 18:52 PM (PST)</td>\n",
       "      <td>VanMan</td>\n",
       "      <td>2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Not what Dodge used to be</td>\n",
       "      <td>Good solid frame and suspension.  Well equipp...</td>\n",
       "      <td>2.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                  Review_Date   Author_Name  \\\n",
       "0   0   on 10/13/05 15:30 PM (PDT)     roadking    \n",
       "1   1   on 07/17/05 21:59 PM (PDT)         Mark    \n",
       "2   2   on 07/16/02 00:00 AM (PDT)    Tom Sheer    \n",
       "3   3   on 12/29/07 21:57 PM (PST)  Keven Smith    \n",
       "4   4   on 02/09/05 18:52 PM (PST)       VanMan    \n",
       "\n",
       "                                       Vehicle_Title  \\\n",
       "0  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "1  2002 Dodge Ram Cargo Van 3500 3dr Ext Van (5.2...   \n",
       "2  2002 Dodge Ram Cargo Van 3500 Maxi 3dr Ext Van...   \n",
       "3  2001 Dodge Ram Cargo Van 2500 Maxi 3dr Ext Van...   \n",
       "4  2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "\n",
       "                Review_Title  \\\n",
       "0     Great delivery vehicle   \n",
       "1             Disappointmnet   \n",
       "2                  Sweet van   \n",
       "3                Keven Smith   \n",
       "4  Not what Dodge used to be   \n",
       "\n",
       "                                              Review  Rating  \n",
       "0   It's been a great delivery vehicle for my caf...   4.625  \n",
       "1   Bought this car as a commuter vehicle for a v...   2.125  \n",
       "2   This van rocks its the best, lots of room. I ...   5.000  \n",
       "3   Great work vehicle. Drives nice. has lots of ...   4.500  \n",
       "4   Good solid frame and suspension.  Well equipp...   2.875  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "## **Part 2: Text Preprocessing**\n",
    "**Objective:** *Gain hands-on experience with text preprocessing techniques.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Import the Necessary Libraries and Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')  # Download Brown Data\n",
    "nltk.download('punkt') # Download Punkt Data\n",
    "nltk.download('stopwords') # Download Stopwords Data\n",
    "nltk.download('wordnet') # Download WordNet Data\n",
    "nltk.download('omw-1.4')  # Download WordNet Data\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Load the Brown Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Now that he knew himself to be self he was free to grok ever closer to his brothers , merge without let . Self's integrity was and is and ever had been . Mike stopped to cherish all his brother selves , the many threes-fulfilled on Mars , corporate and discorporate , the precious few on Earth -- the unknown powers of three on Earth that would be his to merge with and cherish now that at last long waiting he grokked and cherished himself . Mike remained in trance ; ; there was much to grok , loos\n"
     ]
    }
   ],
   "source": [
    "# Load the text from the 'news' category of the Brown Corpus\n",
    "text = ' '.join(brown.words(categories='science_fiction'))\n",
    "print(\"Original Text:\", text[:500])  # Print the first 500 characters for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Tokenization**\n",
    "Tokenization splits the text into individual words and sentences.\n",
    "\n",
    "**Impact of Tokenization:**\n",
    "* Sentence Tokenization: Breaks down the text into manageable units (sentences) for further processing.\n",
    "* Word Tokenization: Provides the basic units (words) needed for subsequent analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sentences: ['Now that he knew himself to be self he was free to grok ever closer to his brothers , merge without let .', \"Self's integrity was and is and ever had been .\", 'Mike stopped to cherish all his brother selves , the many threes-fulfilled on Mars , corporate and discorporate , the precious few on Earth -- the unknown powers of three on Earth that would be his to merge with and cherish now that at last long waiting he grokked and cherished himself .', \"Mike remained in trance ; ; there was much to grok , loose ends to puzzle over and fit into his growing -- all that he had seen and heard and been at the Archangel Foster Tabernacle ( not just cusp when he and Digby had come face to face alone ) why Bishop Senator Boone made him warily uneasy , how Miss Dawn Ardent tasted like a water brother when she was not , the smell of goodness he had incompletely grokked in the jumping up and down and wailing -- Jubal's conversations coming and going -- Jubal's words troubled him most ; ; he studied them , compared them with what he had been taught as a nestling , struggling to bridge between languages , the one he thought with and the one he was learning to think in .\", \"The word `` church '' which turned up over and over again among Jubal's words gave him knotty difficulty ; ; there was no Martian concept to match it -- unless one took `` church '' and `` worship '' and `` God '' and `` congregation '' and many other words and equated them to the totality of the only world he had known during growing-waiting then forced the concept back into English in that phrase which had been rejected ( by each differently ) by Jubal , by Mahmoud , by Digby .\"]\n",
      "First 20 words: ['Now', 'that', 'he', 'knew', 'himself', 'to', 'be', 'self', 'he', 'was', 'free', 'to', 'grok', 'ever', 'closer', 'to', 'his', 'brothers', ',', 'merge']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"First 5 sentences:\", sentences[:5])\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"First 20 words:\", words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Stemming**\n",
    "Stemming reduces words to their root form, stripping suffixes.\n",
    "\n",
    "**Impact of Stemming:**\n",
    "* Reduction of Variants: Words like \"running,\" \"runner,\" and \"ran\" are reduced to \"run,\" which simplifies the text and reduces complexity.\n",
    "* Potential Loss of Meaning: Sometimes, stemming can strip too much, losing the actual meaning of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 stemmed words: ['now', 'that', 'he', 'knew', 'himself', 'to', 'be', 'self', 'he', 'wa', 'free', 'to', 'grok', 'ever', 'closer', 'to', 'hi', 'brother', ',', 'merg']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"First 20 stemmed words:\", stemmed_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Lemmatization**\n",
    "Lemmatization reduces words to their base or dictionary form, considering the context.\n",
    "\n",
    "**Impact of Lemmatization:**\n",
    "* Context-Aware Reduction: More accurate than stemming, as it considers the part of speech and context.\n",
    "* Improved Meaning Preservation: Maintains the integrity of the words better than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 lemmatized words: ['now', 'that', 'he', 'knew', 'himself', 'to', 'be', 'self', 'he', 'wa', 'free', 'to', 'grok', 'ever', 'closer', 'to', 'hi', 'brother', ',', 'merg']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "print(\"First 20 lemmatized words:\", lemmatized_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Stop Word Removal**\n",
    "Stop words are common words (e.g., \"the,\" \"is\") that may not add significant meaning to text analysis.\n",
    "\n",
    "**Impact of Stop Word Removal:**\n",
    "* Noise Reduction: Eliminates common but uninformative words, reducing the size of the text data.\n",
    "* Focus on Meaningful Words: Enhances the focus on significant words that contribute more to the text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words after stop word removal: ['knew', 'self', 'wa', 'free', 'grok', 'ever', 'closer', 'hi', 'brother', 'merg', 'without', 'let', 'self', \"'s\", 'integr', 'wa', 'ever', 'mike', 'stop', 'cherish']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from the tokenized words\n",
    "filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(\"First 20 words after stop word removal:\", filtered_words[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
